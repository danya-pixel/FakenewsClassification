{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake news classification (RU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>is_fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Москвичу Владимиру Клутину пришёл счёт за вмеш...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Агент Кокорина назвал езду по встречке житейск...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Госдума рассмотрит возможность введения секрет...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ФАС заблокировала поставку скоростных трамваев...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Против Навального завели дело о недоносительст...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  is_fake\n",
       "0  Москвичу Владимиру Клутину пришёл счёт за вмеш...        1\n",
       "1  Агент Кокорина назвал езду по встречке житейск...        0\n",
       "2  Госдума рассмотрит возможность введения секрет...        1\n",
       "3  ФАС заблокировала поставку скоростных трамваев...        0\n",
       "4  Против Навального завели дело о недоносительст...        1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('dataset/train.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('dataset/test.tsv', sep='\\t')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/danya-\n",
      "[nltk_data]     sakharov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "mystem_analyzer = Mystem()\n",
    "\n",
    "def base_preprocessing(text):\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = re.sub(f'|'.join([\"»\", \"«\", \"—\"]), '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub('[{}]'.format(punctuation), '', text)\n",
    "    text = mystem_analyzer.lemmatize(text)\n",
    "\n",
    "    return ' '.join([word for word in text if word not in stopwords.words('russian')+[' ', '\\n', \" \"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas_from_text(text_series):\n",
    "    mystem_analyzer = Mystem()\n",
    "    with Pool(8) as pool:\n",
    "        lemmas = list(\n",
    "            tqdm(pool.imap(base_preprocessing, text_series), total=len(text_series)))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Mystem' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/danya-sakharov/KonturDSInterview/fakenews.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bserver/home/danya-sakharov/KonturDSInterview/fakenews.ipynb#ch0000008vscode-remote?line=0'>1</a>\u001b[0m train_data[\u001b[39m'\u001b[39m\u001b[39mlemmas\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m get_lemmas_from_text(train_data[\u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bserver/home/danya-sakharov/KonturDSInterview/fakenews.ipynb#ch0000008vscode-remote?line=1'>2</a>\u001b[0m test_data[\u001b[39m'\u001b[39m\u001b[39mlemmas\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m get_lemmas_from_text(test_data[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bserver/home/danya-sakharov/KonturDSInterview/fakenews.ipynb#ch0000008vscode-remote?line=2'>3</a>\u001b[0m train_data\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;32m/home/danya-sakharov/KonturDSInterview/fakenews.ipynb Cell 7'\u001b[0m in \u001b[0;36mget_lemmas_from_text\u001b[0;34m(text_series)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bserver/home/danya-sakharov/KonturDSInterview/fakenews.ipynb#ch0000006vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_lemmas_from_text\u001b[39m(text_series):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bserver/home/danya-sakharov/KonturDSInterview/fakenews.ipynb#ch0000006vscode-remote?line=1'>2</a>\u001b[0m     \u001b[39mwith\u001b[39;00m Pool(\u001b[39m8\u001b[39m) \u001b[39mas\u001b[39;00m pool:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bserver/home/danya-sakharov/KonturDSInterview/fakenews.ipynb#ch0000006vscode-remote?line=2'>3</a>\u001b[0m         lemmas \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bserver/home/danya-sakharov/KonturDSInterview/fakenews.ipynb#ch0000006vscode-remote?line=3'>4</a>\u001b[0m             tqdm(pool\u001b[39m.\u001b[39;49mimap(base_preprocessing, text_series, mystem_analyzer), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_data)))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bserver/home/danya-sakharov/KonturDSInterview/fakenews.ipynb#ch0000006vscode-remote?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m lemmas\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:407\u001b[0m, in \u001b[0;36mPool.imap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=404'>405</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=405'>406</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=406'>407</a>\u001b[0m     \u001b[39mif\u001b[39;00m chunksize \u001b[39m<\u001b[39;49m \u001b[39m1\u001b[39;49m:\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=407'>408</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=408'>409</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mChunksize must be 1+, not \u001b[39m\u001b[39m{0:n}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=409'>410</a>\u001b[0m                 chunksize))\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=410'>411</a>\u001b[0m     task_batches \u001b[39m=\u001b[39m Pool\u001b[39m.\u001b[39m_get_tasks(func, iterable, chunksize)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'Mystem' and 'int'"
     ]
    }
   ],
   "source": [
    "train_data['lemmas'] = get_lemmas_from_text(train_data['title'])\n",
    "test_data['lemmas'] = get_lemmas_from_text(test_data['title'])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train examples 3742\n",
      "total test examples 2016\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    train_data['lemmas'], train_data['is_fake'], test_size=0.35)\n",
    "\n",
    "print(\"total train examples %s\" % len(y_train))\n",
    "print(\"total test examples %s\" % len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2879\n",
       "0    2879\n",
       "Name: is_fake, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.is_fake.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('title',axis=1, inplace=True)\n",
    "test_data.drop('title', axis=1, inplace=True)\n",
    "train, val = train_test_split(train_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('dataset/preprocessed/train.csv')\n",
    "val.to_csv('dataset/preprocessed/val.csv')\n",
    "test_data.to_csv('dataset/preprocessed/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fasttext/data.train.txt', 'w+') as outfile:\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        outfile.write('__label__' + str(y) + ' ' + x + '\\n')\n",
    "\n",
    "\n",
    "with open('fasttext/test.txt', 'w+') as outfile:\n",
    "    for x, y in zip(x_val, y_val):\n",
    "        outfile.write('__label__' + str(y) + ' ' + x + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  8392\n",
      "Number of labels: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8447420634920635\n",
      "Recall: 0.8447420634920635\n",
      "Number of examples: 2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:  443181 lr:  0.000000 avg.loss:  0.030531 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "classifier = fasttext.train_supervised(\n",
    "    'fasttext/data.train.txt', lr=1.0, epoch=25, wordNgrams=3)\n",
    "num, presicion, recall = classifier.test('fasttext/test.txt')\n",
    "\n",
    "print(f'Precision: {presicion}')\n",
    "print(f'Recall: {recall}')\n",
    "print('Number of examples:', num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_NAME = \"DeepPavlov/bert-base-bg-cs-pl-ru-cased\"\n",
    "MODEL_NAME = \"DeepPavlov/bert-base-bg-cs-pl-ru-cased\"\n",
    "SAVED_MODEL_NAME = MODEL_NAME.split(\"/\", 1)[1]\n",
    "MAX_SEQ_LENGTH = 200\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BertInputItem(object):\n",
    "    \"\"\"An item with all the necessary attributes for finetuning BERT.\"\"\"\n",
    "\n",
    "    def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.text = text\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "def convert_examples_to_inputs(example_texts, example_labels, max_seq_length, tokenizer, verbose=0):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    input_items = []\n",
    "    examples = zip(example_texts, example_labels)\n",
    "    for (ex_index, (text, label)) in enumerate(examples):\n",
    "\n",
    "        # Create a list of token ids\n",
    "        input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n",
    "        if len(input_ids) > max_seq_length:\n",
    "            input_ids = input_ids[:max_seq_length]\n",
    "\n",
    "        # All our tokens are in the first input segment (id 0).\n",
    "        segment_ids = [0] * len(input_ids)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        input_items.append(\n",
    "            BertInputItem(text=text,\n",
    "                          input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label))\n",
    "\n",
    "    return input_items\n",
    "\n",
    "\n",
    "train_features = convert_examples_to_inputs(\n",
    "    x_train, y_train, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n",
    "val_features = convert_examples_to_inputs(\n",
    "    x_val, y_val, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def get_data_loader(features, batch_size, shuffle=True):\n",
    "\n",
    "    all_input_ids = torch.tensor(\n",
    "        [f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(\n",
    "        [f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(\n",
    "        [f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor(\n",
    "        [f.label_id for f in features], dtype=torch.long)\n",
    "    data = TensorDataset(all_input_ids, all_input_mask,\n",
    "                         all_segment_ids, all_label_ids)\n",
    "\n",
    "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "train_dataloader = get_data_loader(train_features, BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = get_data_loader(val_features, BATCH_SIZE, shuffle=False)\n",
    "#test_dataloader = get_data_loader(test_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/bert-base-bg-cs-pl-ru-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/bert-base-bg-cs-pl-ru-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.bert.modeling_bert import BertForSequenceClassification\n",
    "\n",
    "#config = BertConfig.from_pretrained(\"DeepPavlov/rubert-base-cased\", num_labels=1)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "out_features = model.bert.encoder.layer[1].output.dense.out_features\n",
    "model.classifier = torch.nn.Linear(out_features, 2)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "NUM_TRAIN_EPOCHS = 35\n",
    "LEARNING_RATE = 1e-5\n",
    "WARMUP_PROPORTION = 0.1\n",
    "MAX_GRAD_NORM = 5\n",
    "\n",
    "num_train_steps = int(len(train_dataloader.dataset) /\n",
    "                      BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(WARMUP_PROPORTION * num_train_steps)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    predicted_labels, correct_labels = [], []\n",
    "\n",
    "    for _, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=input_mask,\n",
    "                            token_type_ids=segment_ids, labels=label_ids)\n",
    "        logits = outputs[1]\n",
    "\n",
    "        eval_loss = loss_fn(outputs.logits, label_ids)\n",
    "\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        outputs = np.argmax(logits.to('cpu'), axis=1)\n",
    "\n",
    "        predicted_labels += list(outputs)\n",
    "        correct_labels += list(label_ids)\n",
    "\n",
    "        eval_loss += eval_loss.item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "    correct_labels = np.array(correct_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "    return eval_loss, correct_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "OUTPUT_DIR = \"trained_models/\"\n",
    "PATIENCE = 4\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "no_improvement = 0\n",
    "# for _ in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
    "#     model.train()\n",
    "#     tr_loss = 0\n",
    "#     nb_tr_examples, nb_tr_steps = 0, 0\n",
    "#     for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "#         outputs = model(input_ids, attention_mask=input_mask,\n",
    "#                         token_type_ids=segment_ids, labels=label_ids)\n",
    "#         loss = outputs[0]\n",
    "\n",
    "#         preds = torch.argmax(outputs.logits, dim=1)\n",
    "#         loss = loss_fn(outputs.logits, label_ids)\n",
    "\n",
    "#         if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "#             loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "#         loss.backward()\n",
    "#         tr_loss += loss.item()\n",
    "\n",
    "#         if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "#             scheduler.step()\n",
    "\n",
    "#     dev_loss, dev_correct, dev_predicted = evaluate(\n",
    "#         model, val_dataloader, loss_fn)\n",
    "#     dev_acc = np.mean(dev_predicted == dev_correct)\n",
    "\n",
    "#     if (len(loss_history) > 3):\n",
    "#         print(f\"Loss history: {loss_history[-3:]}\")\n",
    "#     else:\n",
    "#         print(f\"Loss history: {loss_history}\")\n",
    "#     print(f\"Dev loss: {dev_loss}\")\n",
    "#     print(f\"Dev accuracy: {dev_acc}\")\n",
    "\n",
    "#     if len(acc_history) == 0 or dev_acc > max(acc_history):\n",
    "#         print('New record, model saved')\n",
    "#         no_improvement = 0\n",
    "#         model_to_save = model.module if hasattr(model, 'module') else model\n",
    "#         output_model_file = os.path.join(OUTPUT_DIR, SAVED_MODEL_NAME)\n",
    "#         torch.save(model_to_save.state_dict(), output_model_file)\n",
    "#     elif dev_acc < acc_history[-1]:\n",
    "#         no_improvement += 1\n",
    "\n",
    "#     if no_improvement > PATIENCE:\n",
    "#         print(\"No improvement on development set. Finish training.\")\n",
    "#         break\n",
    "\n",
    "#     loss_history.append(dev_loss.item())\n",
    "#     acc_history.append(dev_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065190ed36da447e9f75aff1277d439f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147b745bb3824f25b2a144687f7e148e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training performance: (1.0, 1.0, 1.0, None)\n",
      "Development performance: (0.8988095238095238, 0.8988095238095238, 0.8988095238095238, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90       989\n",
      "           1       0.89      0.91      0.90      1027\n",
      "\n",
      "    accuracy                           0.90      2016\n",
      "   macro avg       0.90      0.90      0.90      2016\n",
      "weighted avg       0.90      0.90      0.90      2016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = torch.load(os.path.join(OUTPUT_DIR, SAVED_MODEL_NAME), map_location=lambda storage, loc: storage)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, state_dict=model_state_dict)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "_, train_correct, train_predicted = evaluate(model, train_dataloader, loss_fn)\n",
    "_, dev_correct, dev_predicted = evaluate(model, val_dataloader, loss_fn)\n",
    "\n",
    "print(\"Training performance:\", precision_recall_fscore_support(\n",
    "    train_correct, train_predicted, average=\"micro\"))\n",
    "print(\"Development performance:\", precision_recall_fscore_support(\n",
    "    dev_correct, dev_predicted, average=\"micro\"))\n",
    "\n",
    "bert_accuracy = np.mean(dev_predicted == dev_correct)\n",
    "\n",
    "print(classification_report(dev_correct, dev_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ученый ради марс проживать восемь месяц изоляция гавайи 0\n",
      "москвич пригрозить останавливать работа старый McDonalds россия 0\n",
      "основной кормилец процент семья сша оказываться женщина 0\n",
      "бритни спирс обвинять жестокий обращение ребенок 0\n",
      "искусствовед картина мона лиза изображать мать иван грозный 1\n",
      "огненный шоколад заинтересовывать российский орган власть 0\n",
      "медведев потребовать продавать Twitter российский компания 1\n",
      "кудрин призывать перевести россия ветроэнергетика прямо 1\n",
      "камасутра признавать олимпийский вид спорт 1\n",
      "доллар рухнуть отказ ФНБ американский валюта 1\n",
      "россия вступать сила мораторий действие довсе 0\n",
      "борьба безработица правительство отменять социальный пособие 1\n",
      "экономия кислород мкс катапультировать весь женщина 1\n",
      "гендиректор белавиа самолет оправдывать 1\n",
      "лондонский конкурс двойник мистер бин побеждать аутист саратов 1\n",
      "летний аргентинка забеременеть прививка спутник V 1\n",
      "пол маккартень приглашать свой концерт виртуальный реальность 0\n",
      "анатолий чубайс возглавлять сбербанк 1\n",
      "госдеп обязывать ведущий российский компания создавать представительство сша 1\n",
      "олимпийский чемпионка журова проводить экскурсия школьник 0\n",
      "сша перепрограммировать ген возбудитель сальмонеллез 0\n",
      "казань зафиксировать балльный пробка тротуар 1\n",
      "греф решать наказывать зицпредседатель 0\n",
      "Dead Rising добавлять миссия спасение президент 0\n",
      "греф россия вступать вто конец год 0\n",
      "корабль адмирал чабаненко входить панамский канал 0\n",
      "россия уменьшать размер презерватив избегать рост цена 1\n",
      "лукойл подавать суд перекраска заправка сша 0\n",
      "хабаровский зоопарк рысь обматерить посетитель 1\n",
      "коллекция музей азлк воссоединяться собрание автоссср тула 1\n",
      "малайзия красавица чудовище вырезать гейперсонаж отменять премьера 0\n",
      "эмблема олимпиада токио заменять изза обвинение плагиат 0\n",
      "певец юркисс становиться автор новый гимн россия 1\n",
      "госдума запрещать экспорт цветной металл россия 0\n",
      "баннер портрет собчак нецензурный слово появляться москва 0\n",
      "стрелять юрий лоза прокомментировать инцидент британский эсминец черный море 1\n",
      "международный олимпийский комитет признавать неэффективность антисекскровать токио 1\n",
      "новый версия Google Chrome работать пк оперативный память менее гигабайт 1\n",
      "Apple запатентовывать инновационный технология досмотр человек собственный телефон 1\n",
      "великий новгород находить фреска круг феофан грек 0\n",
      "визит трамп завод HarleyDavidson отменять изза угроза протест 0\n",
      "россиянка входить рейтинг самый влиятельный бизнеслеживать мир 0\n",
      "состав бундестаг согласовываться российский представитель 1\n",
      "беглов посоветовать власть мадрид убирать снег 1\n",
      "магаданский заповедник волк загрызать краснокнижный орлан 0\n",
      "россиянин запрещать смотреть кортеж 1\n",
      "электронный паспорт прийти помощь наемный убийца 0\n",
      "росавиация хотеть регламентировать полет пчела 1\n",
      "москва создавать комитет управление гостиница 0\n",
      "китай становиться крупный экспортер береза мир 1\n",
      "справедливый перераспределение акция американский компания заниматься госплан сша 1\n",
      "Lego выпускать ограниченный серия Bionicle долгополов соловьев 1\n",
      "кировский филармония организовывать концерт глухой ребенок 1\n",
      "пресссекретарь лужков обвинять дворкович незнание московский жизнь 0\n",
      "парусный регата турция собирать шкипер первый пятерка рейтинг 0\n",
      "альфабанк начинать продавать карта красивый номер 1\n",
      "самый сильный девушка мир прибывать сирия приглашение жена асад 0\n",
      "някрошюс поставлять песнь песнь ветхий завет 0\n",
      "робот федор появляться Battlefield  \n",
      " 1\n",
      "тринадцать польский аквалангист пострадать попытка демонтаж северный поток 1\n",
      "подсчитывать потеря аэропорт запрет полет украина египет турция 0\n",
      "ученый опровергать влияние детский травма жизнь синелапый олуши 0\n",
      "госплан рф снижать цена основной продукт питание 1\n",
      "русский премия доставаться украинец киргиз 0\n",
      "госплан рф уверенный масштабный голод россия грозить 1\n",
      "подмосковье жестоко избивать музыкант группа любэ 0\n",
      "китай вкладывать миллиард доллар газопровод сила сахара 1\n",
      "армия фанатка BTS занимать место боеспособность мир 1\n",
      "россия налаживать экспорт шпроты латвия 1\n",
      "росавиация разрешать самолет победа летать двигатель 1\n",
      "белорусский суд признавать процент российский кредит завышать 1\n",
      "МПК заинтересовываться личность пронести флаг россия белорусский паралимпиец 0\n",
      "лорел хаббард объяснять неудача олимпиада беременность 1\n",
      "суд попросить запрещать кехман уезжать граница 0\n",
      "борьба здоровый грудь включаться розовый фотоаппарат Polaroid 0\n",
      "фотомессенджер Facebook Slingshot становиться доступный россия 0\n",
      "президент франция оказываться автор эротический роман 0\n",
      "словакия отказывать украина реверсный поставка газ северный поток 1\n",
      "российский ученый находить спусковой крючок болезнь альцгеймер 0\n",
      "пенсионный фонд россия становиться частный компания 1\n",
      "никсельпиксел изготавливать эксклюзивный ковер шерсть свой нога 1\n",
      "суд оставлять александр емельяненко тюрьма ноябрь 0\n",
      "михаил горбачев встречаться кремль гельмут коль съемка телесериал 0\n",
      "Mash удалять кадр золотой батон видео дворец геленджик 1\n",
      "год ес изымать поддельный виагра сигарета миллиард евро 0\n",
      "норвежский олимпиец отдавать свой призовой пенсионер 1\n",
      "клуб роман павлюченко запрещать вувузел 0\n",
      "войско нато отмечать летие победа холодный война грандиозный военный парад 1\n",
      "порошенко призывать рада одобрять реформа мвф 0\n",
      "бывший полковник гру раскрывать незаконный схема заработок сотрудник разведка 1\n",
      "объявлять длинный список премия русский букер 0\n",
      "студент гнесинка опротестовывать выселение общежитие концерт 0\n",
      "украина представлять эмблема слоган евровидение 0\n",
      "сын че гевара мечтать съемка кубинский че 0\n",
      "подмосковье получать компенсация расширение москва 0\n",
      "ученый лишаться подписка тысяча журнал изза обвал рубль 0\n",
      "совет безопасность собираться обсуждение цена свекла 1\n",
      "аэрофлот сделать женщинпилот пилотка 1\n",
      "ананд смочь обыгрывать крамник второй партия матч первенство мир шахматы 0\n",
      "французский ученый оценивать ущерб наносить африка выпускник мгимо 1\n",
      "американский адмирал пообещать принуждать европа трансатлантический сотрудничество 1\n",
      "адам сандлер организовывать секс телефон 0\n",
      "воронеж размах отмечать юбилейный прорыв теплотрасса 1\n",
      "теодора рузвельт продлять срок служба 0\n",
      "дальавиа прекращать выполнять свой рейс москва санктпетербург 0\n",
      "чубайс объяснять желание прильнуть пенсия россиянин 0\n",
      "ассоциация пивоваров чехия официально признавать российский пиво моча 1\n",
      "факел олимпийский эстафета побывать открытый космос 0\n",
      "пласидо доминго петь кремль соборный красный площадь 0\n",
      "новый тренер сборная россия становиться зарема салихов 1\n",
      "тува обнаруживать сохраняться захоронение скиф 0\n",
      "выбирать сценарист новый версия лицо шрам 0\n",
      "продлять срок прием предзаказ танцующий роботапаук 0\n",
      "роскачество замачивать сжигать мужской носок Pierre Cardin Modis 0\n",
      "мэр содом гоморра отказываться поддерживать месяц гордость 1\n",
      "покупатель футболка сборная украина оставаться подписка невыезд 1\n",
      "суд отклонять иск навальный оператор система платон 0\n",
      "взрыв батарея мобильник убивать китаец 0\n",
      "алексей миллер валентина матвиенко подарить зенит новый стадион 0\n",
      "чиновник разрешать сбивать пешеход чрезвычайный обстоятельство 1\n",
      "белорусский омон принимать вооружение новый ЗРК букм 1\n",
      "швеция выходить первый место мир использование IT 0\n",
      "вуди аллен террорист добиваться обратный результат 0\n",
      "фонд билл гейтс выделять полмиллиард доллар банковский система бедный 0\n",
      "бризм смочь вывести расчетный орбита спутник связь Arabsat A 0\n",
      "ржд отменять электричка регион 0\n",
      "режиссер геликонопер предлагать перебираться швеция 0\n",
      "гувд московский область обнародоваться итог рокфестиваль нашествие 0\n",
      "геофизик создавать алмазный камера моделирование мантия земля 0\n",
      "сотня неизвестный область обнаруживать мозг человек 0\n",
      "цена табачный изделие мочь вырастать  \n",
      " 1\n",
      "иерусалим торжественно открывать памятник сталин 1\n",
      "московский подземка появляться первый кафе 0\n",
      "комитет оон изучение потепление заниматься расследование климатгейт 0\n",
      "правительство называть срок новогодний выплата пенсионер 1\n",
      "олег саленко продавать золотой бутса 0\n",
      "NASA принимать решение возвращать мкс астронавт голосовать трамп 1\n",
      "реалитишоу дом переезжать телеканал культура 1\n",
      "прибор наведение противотанковый ракета приспособлять борьба малярия 0\n",
      "словарь ABBYY Lingvo получать функция видеоперевод 0\n",
      "алтай девочка съедать волк 1\n",
      "ростовский милиция применять против спартаковский болельщик дубинка слезоточивый газ 0\n",
      "белорусский зайчик появляться графический символ 0\n",
      "покровский ик носить имя андрей сахаров 1\n",
      "ромарио получать разрешение забивать тысячный гол маракан 0\n",
      "раскопка каир находить первый история финансовый пирамида 1\n",
      "людмила путин потребовать половина дворец геленджик 1\n",
      "поисковик рейс Routehappy оценивать качество WiFi борт аэрофлот 0\n",
      "певица ника минаж оплачивать студент бишкек поездка фестиваль сочи 0\n",
      "госдума ограничивать реклама жилищный строительство 0\n",
      "робот Boston Dynamics создавать профсоюз 1\n",
      "череповец впервые проходить всероссийский фестиваль настойка боярышник 1\n",
      "украинец пообещать льготный кредит покупка негазовый котел 0\n",
      "австралия присоединяться запрет кассетный боеприпасы 0\n",
      "конгресс сша навязывать рамсфелд тысяча солдат 0\n",
      "банк венесуэла разрешать обращение чернобелый копия деньги 1\n",
      "парламент украина принимать поправка янукович налоговый кодекс 0\n",
      "засуживать российский спортсмен обращаться мосгорсуд 1\n",
      "EA анонсировать дополнение дворец игра The Sims  \n",
      " 1\n",
      "дума вносить законопроект внутренний интернет 1\n",
      "ягр называть силовой прием овечкин нелегальный 0\n",
      "житель омск попытаться продавать свой часть крым 1\n",
      "британец снимать видео создавать огненный торнадо 0\n",
      "трампизм официально вносить международный классификация болезнь 1\n",
      "терновый венец иисус замироточить нефть 1\n",
      "полгода краснодарский край снести объект самострой 0\n",
      "прививка AstraZeneca скончаться кощей бессмертный 1\n",
      "учитель отказываться вырезать голый грудь матильда 0\n",
      "гимнаст нагорный рассказывать победа многоборье кубок россия 0\n",
      "логистический оператор пообещать возобновлять доставка посылка россиянин 0\n",
      "называть топ самый бесполезный подарок март 1\n",
      "уткин потребовать канделак извинение сообщение соцсеть 0\n",
      "курт кобейн оказываться противник белый рэп 0\n",
      "сми половина французский министр тайно прививаться спутник V 1\n",
      "ройзман рассказывать принимать участие акция поддержка путин 1\n",
      "росприроднадзор попросить прокуратура проверять митволя 0\n",
      "российский спортсмен выступать олимпиада Unknown athletes 1\n",
      "минтранс утверждать тариф пользование эскалатор 1\n",
      "сокуров извиняться извинение путин 1\n",
      "никита михалкова лишать оскар гомофобный высказывание 1\n",
      "старый передвижной цирк сша закрываться изза зоозащитник 0\n",
      "турция признавать геноцид коренной народ сша 1\n",
      "правозащитник попросить оон защищать куратор запретный искусство 0\n",
      "милон потребовать пускать океан ельзь россия 0\n",
      "аэрофлот временно прекращать полет изза нехватка нарколог 1\n",
      "петровский путевой дворец открывать ко день город 0\n",
      "московский антипрививочник изолировать стадион лужники 1\n",
      "россия законодательно ограничивать жирность молоко масло сметана 1\n",
      "джордж сорос становиться почетный профессор ВШЭ 1\n",
      "становиться мемой бюст роналд исчезать португальский аэропорт 0\n",
      "спасение малярия медик посоветовать спать курица 0\n",
      "устанавливать главный опасность здоровье просмотр телевизор 0\n",
      "депутат госдума призывать колосков ответ португальский позор 0\n",
      "Domestos разрабатывать свой антивирус 1\n",
      "британский студент становиться предупреждать сексуальный насилие пьеса шекспир 0\n",
      "линус торвальдс сообщать грядущий монетизация ядро Linux 1\n",
      "захаров придумывать евровидение слоган говорить поукраински дебил 0\n",
      "мединский потребовать прекращать шизофренический рефлексия тема история 0\n",
      "заказчик скульптура против гендерный неравенство оштрафовать сексизм сша 0\n",
      "роскосмос признаваться заблаговременный доставка знамя победа мкс 0\n",
      "пресный вода крым закупать израиль 1\n",
      "залп донецкий рсзо снежинка чебурашка попадать видео 0\n",
      "грузинский морковка дыня пускать россия 0\n",
      "центральный офис CocaCola оправдываться публикация карта россия крым 0\n"
     ]
    }
   ],
   "source": [
    "incorrect_samples = []\n",
    "for feature, predict in zip(val_features, list(dev_correct != dev_predicted)):\n",
    "    if predict:\n",
    "        print(feature.text, feature.label_id) \n",
    "        incorrect_samples.append(feature.text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b705933523dbfb6c667215fc43ae1e977badfceb5917d06dffc112ad1d263028"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('nlp': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
